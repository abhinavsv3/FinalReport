% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{report}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{afterpage}
\usepackage{eurosym}
\usepackage[colorinlistoftodos]{todonotes}
\setcounter{secnumdepth}{5}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\newfloat{algorithm}{t}{lop}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\newcommand*{\addheight}[2][.5ex]{%
  \raisebox{0pt}[\dimexpr\height+(#1)\relax]{#2}%
}

\newenvironment{dedication}
  {
\clearpage           % we want a new page
   \thispagestyle{empty}% no header and footer
   \vspace*{\stretch{1}}% some space at the top 
   \itshape             % the text is in italics
   \raggedleft          % flush to the right margin
  }
  {\par % end the paragraph
   \vspace{\stretch{3}} % space at bottom is three times that at the top
   \clearpage           % finish off the page
  }
\lstset{
    frame=single,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}
\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Universitat Politècnica de Catalunya}\\[2.5cm] % Name of your university/college
\textsc{\Large Bachelors in Computer Science and Engineering}\\[0.5cm] % Major heading such as course name
%\textsc{\large Minor Heading}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Graph and matrix algorithms for visualizing high dimensional data}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Director:}\\
Dr. Ricard Gavalda Mestre 
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Co-Director:} \\
Dr. Marta Arias Vicente % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Bachelors Thesis of :}\\
Abhinav Shankaranarayanan Venkataraman
\end{flushleft}
\end{minipage}

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------
\bigskip
\bigskip
{\large June 20,2016}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics{logo.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\newpage




\begin{dedication}
To my Mother, Father, Professors and Friends. I owe a lot to My professors Ricard Gavalda and Marta Arias and to Babaji at Gurudwara
\end{dedication}
\newpage
\clearpage
\newpage
\newpage

\begin{abstract}
Motivated by the problem of understanding data from
the medical domain, we consider algorithms for visually representing 
highly dimensional data so that "similar" entities appear close together. We will study, 
implement and compare several algorithms based on graph and on matrix
representation of the data. The first kind are known as "community detection"
algorithms, the second kind as "clustering" algorithms. The implementations
should be robust, scalable, and provide a visually appealing representation
of the main structures in the data.

\end{abstract}

\newpage
\clearpage
\newpage
\section*{Acknowledgement}
I would like to Acknowledge the support provided by my faculty and admins at my home university -- SASTRA University, Thanjavur and UPC Barcelona for supporting me throughout the project.

\newpage

\tableofcontents
\newpage

\chapter{Introduction}

\setcounter{page}{1}
\par In this section an entire overview of the full project is provided. We mention the context of the project we have studied and the goal of the project. We also provide the intended planning, economic estimate and sustainability of the work that has been done.


\section{Context Of the Project}
\par In the present day scenario, the modern science of algorithms and graph theory has brought significant advances to our understanding of complex data. Many complex systems are representable in the form of graphs. Graphs have time and again been used to represent real world networks. One of the most pertinent feature of graphs representing real system is community structures or otherwise known as clusters. Community can be defined as the organization of vertices in groups or clusters, with many edges joining the vertices of the same cluster and comparatively fewer vertices joining the vertices in another neighbouring cluster. Such communities form an independent compartment of a graph exhibiting similar role.
Thus, community detection is the key for understanding the structure of complex graphs, and ultimately deduce information from them.

\par The networks and highly dimensional data that motivate this problem emerge from the healthcare domain, and particularly from the analysis of complex, chronic disease, which is the major cost factor in modern societies. In the current scenario, a patient does not have one disease but a
set of diseases. For example a person with diabetes has a heart disease, kidney disease, high blood
pressure etc. This may vary between sexes, ages etc and thus is a very complex landscape to
explore. Visualizing this landscape of diseases would help to analyse the source, the treatment
and even the path way of research to done. Thus, such a visualization would be helpful for the
medical experts and health planner to understand the landscape of diseases much better.
\par Within the perspective of the LARCA project, two kinds of networks could be useful to study in this scneario: one in which nodes are patients and edges indicate their similarity, and another one in which nodes are diagnostics/diseases, and edges indicate their association in a population. Hence, we address this visualization of such high dimensional data using the algorithms and visualization technologies. LARCA has been involved in  healthcare project by LARCA, a publication of the same has been made in the DSAA conference \cite{sirlarca} . A few solutions that are possible to resolve the problem will be analysed and tests will be conducted. The project will also involve study of various algorithms and their respective analysis based on the quality and quantity of data using multiple appropriate experiments. Although, the project is motivated by the real high dimensional data it is not easy to get such data and hence would use simpler ones for testing the project.  


%\paragraph{General usefulness of community detection}
%The ability to detect groups or communities can be significant practical importance for example, a groups of world wide web (WWW) can lead to web pages on related topics, group of social network can correspond to social units or communities that share common interest.Simply finding that a graph contains tightly knit groups at all can convey useful information if a metabolic network were divided into such groups, for instance, it could provide evidence for a modular view of the network's dynamics, with different groups of nodes performing different functions with some degree of independence. There are two tempting methods that have a long history of being studied. One is graph partitioning and the other one being hierachycal cluster. Both these domains address the same question of spliting a task into communities.

\section{Goal of the Project}
The project is built with due recommendations from the director of the project. The project is aimed at using medical domain and thus slides to the side of implemenation which involves faster computation for better visualization. Hence, there are 4 facets or goals for the project which are enumerated as below:
\begin{enumerate}
\item First objective of the project is to survey a few algorithms that aim in community finding keeping in mind that the input is from the medical domain 
\item Next, to choose couple of algorithms that benefit the purpose of graphs from medical domain and for the purpose of visualization.
\item Implement the algorithms and test the efficiency of the algorithm using variety of graphs.
\item Lastly but more importantly to build a Graphic User Interface (GUI) which enables visualization of the raw input on a web browser by drawing graphs.

\end{enumerate}
\section{Planning}
Planning is essential component of any project. It helps to keep pace with the time.
The total duration of the project is 5 months starting from early February 2016 to the end of June 2016. The following describes the tasks that were planned to be performed in the project.
\subsubsection{Task Description}
The tasks for the project have been subdivided into various task phases which are enumerated below : 
\begin{itemize}
\item \textbf{Required knowledge acquisition}\\
Necessary knowledge to understand the problem needs to be gained in order to deal with the original topic.In this phase we familiarize with the term of community detection, graph theory and understand all the possible methods that are in practice to deal with the problem. Knowledge about a few visualization methods is also necessary to implement the visualization of the project.
\item \textbf{Paper Analysis}\\ 
Analysis of paper related to community detection and clustering algorithms over high dimentional graph data is done in this phase of the project. This phase is necessary to understand various functionalities that the project deals with and to assist in the subsequent phases of the project.
\item \textbf{Design and Implementation} \\
The required functionalities are listed and implemented using a programming language. In this phase the methods of the project are designed and programmed using the choose language. 
The implementation is done for both the community detection algorithm and for the visualization aspect of the project.
\item \textbf{Testing I}\\
In this phase the program is tested using generated test cases and errors are identified and corrected. Multiple recoding is done in this phase of the project
In this phase we test the program in order to identify errors in
the implementation. It includes the successive recoding.
\item \textbf{Testing II}\\
In this phase we perform tests are performed on the GUI to ensure the limits of GUI. 
\item \textbf{Report Writing}\\
In this phase the report of the project is written.
\end{itemize} 
\section{Economic Budget}
\subsection{An Introduction to Economic Budget}
Economic management is primarily based on an estimate of income and expenditure called as
budget. Development of a sustainable budget leads to proper economic management of the
project. Budget and sustainability is one of the most important phase of the project
management. In this phase we analyse the budget for the project. We also aim at providing an
estimate of the project budget and optimize the same. We look at the expenditure from various
aspects such as software costs, hardware costs, license costs and human resource costs.
Additionally we also account the software for its sustainability. One important factor to note is
that the budget that we describe in this section is subject to change and it may increase
depending on the unexpected obstacles that we may face. For an instance when we don’t get
the expected results with a particular software we may have to go in for another software that
may incur extra installation and operational charges.
\subsection{Estimation of Economic Budget}
We divide the overall expenditure into three categories namely hardware, software and human
resources. One very important factor that we need to consider is that we only get an estimate
of the total cost. This may vary depending on the systems in use. To calculate the amortization
we consider to factors namely, first the overall life of the hardware or software in use. Second
that the project is completed in 5 months. Hence the amortization cost comes one eighth
of the actual life of the component.\\

\subsubsection{Hardware Budget} Hardware budget accounts for the actual and the amortized costs of the hardware elements
used by the project. The cost is fictitious as it has not been developed commercially. Table 1.
intents to estimate the economic cost of each of the hardware component of the project.
\\\\
\begin{tabular}{|p{1cm}||p{3cm}|p{2cm}|p{3cm}|p{3cm}|}
 \hline
 \multicolumn{5}{|c|}{Table 1 - Hardware Budget} \\
 \hline
 Sno: & Hardware Component&Useful Life(in years) &Total Cost(in \euro) &Amortized Cost(in \euro)\\
 \hline
1   & PC System  &4 &  1000\euro  & 125 \euro \\
\hline
\hline
   & \textbf{Total}  &  &  \textbf{1000}\euro  & \textbf{125} \euro \\
 \hline

\end{tabular}

\subsubsection{Software Budget}
The software budget shows an estimate for the various software used in the project along with
the estimate of the software costs. It is a myth that the software doesn’t get old with time just
as a software gets but it wears out with time. Thus for every software there is a fixed time
during which it gives maximum performance. In addition freeware software and open source
software incur no cost. The cost is fictitious as it has not been developed commercially. Table 2
intents to estimate the economic cost of each of the software component of the project.
\\ \\
\begin{tabular}{|p{1cm}||p{3cm}|p{2cm}|p{3cm}|p{3cm}|}
 \hline
 \multicolumn{5}{|c|}{Table 2 - Software Budget} \\
 \hline
 Sno: & Software Component&Useful Life(in years) &Total Cost(in \euro) &Amortized Cost(in \euro)\\
 \hline
1   & Linux OS  &5 &  0\euro  & 0 \euro \\
2   & JavaScript Engine  &1 &  0\euro  & 0 \euro \\
3   & Python Components  &1 &  0\euro  & 0 \euro \\
4   & Web.py  &1 &  0\euro  & 0 \euro \\
5   & TexMaker  &1 &  0\euro  & 0 \euro \\

\hline
\hline
   & \textbf{Total}  &  &  \textbf{0}\euro  & \textbf{0} \euro \\
 \hline
\end{tabular}
\\ \\ 
\subsubsection{Human Resource Budget}
The human resource budget deals with the overall expenditure spent on human resources.
Every phase of the project has a cost associated with it in per hour calculation.
The cost is fictitious as it has not been developed commercially. Table 3 intents to estimate the
economic cost of each of the phases of the project. The cost per hour is intended as an
approximation of the current cost per work hour of young analysts and developers in our
environment.\\

\begin{tabular}{|p{0.8cm}||p{4cm}|p{2.5cm}|p{1cm}|p{1.5cm}|p{2cm}|}
 \hline
 \multicolumn{6}{|c|}{Table 3 - Human Resource Budget} \\
 \hline
 Sno: & Phase&Deadline &Hours &Cost(per hour in \euro)&Total(in \euro)\\
 \hline
1   & Required Knowledge Acqusition  &1 Mar 2016 &  70  & 15\euro/h & 1050 \euro \\
2   & Paper Analysis  &1 Apr 2016& 150 &  15\euro/h  & 2250 \euro \\
3   & Design and Implementation&30 Apr 2016 &230&  20\euro/h  & 4600 \euro \\
4   & Testing I  &15 May 2016&75 &15\euro/h  & 0 1125\euro \\
5   & Testing II  &31 May 2016&75 &  15\euro/h  & 0 1125\euro \\
6   & Report Writing  &15 Jun 2016&100 &  15\euro/h  & 1500\euro \\
\hline
\hline
   & \textbf{Total}  &  & \textbf{600}&   & \textbf{10525} \euro \\
 \hline
\end{tabular}

\subsubsection{Total Budget}
The following table, Table 4, summarizes the total budget for the project. This encompasses the
hardware, software and human resources budget.
\\ \\ 
\begin{tabular}{|p{1cm}||p{4cm}|p{3cm}|}
 \hline
 \multicolumn{3}{|c|}{Table 4 - Total Budget} \\
 \hline
 Sno: &Resource &Total Cost(in \euro) \\
 \hline
 1   & Hardware Budget  & 1000 \euro \\
 2   & Software Budget  & 0 \euro \\
 3   & Software Budget  & 10525 \euro \\
 \hline
\hline
   & \textbf{Total}  & \textbf{11525} \euro \\
 \hline
 
 

\end{tabular}


\section{Sustainability}
Sustainability is a key factor in any project design. We evaluate the project based on three
factors of sustainability namely economic sustainability, social sustainability and environmental
sustainability.
\subsection{Economic Sustainability}
 In this document we specify
the budget estimation of the project. From our estimation it can be said that this will be the
maximum bound on the budget for the project. This takes into account all the factors namely
the hardware costs, software costs and human resource costs. The cost estimated in the project is the least possible cost and hence is a nonpareil project estimate for any indistinguishable project. The budget
may exceed our calculations only during unexpected times. When the proposed plan is precisely followed the estimated lower costs gets achieved. 
Also the product that we aim at developing here is tested with all kinds of data and we aim at
building a very high quality software which in turn provides a durable software that will not
wear out easily.  Most of the software used in the project is open source which has zero product cost. The The hardware required is nothing but computers that becomes a mandatory part
of any project in the present days.

\subsection{Social Sustainability}
The project aims at developing web based platform to perform learning cum visualization
analytics. This is indirectly going to analyze the learning characteristics of the patients and
provide a feedback both to the medical analyzer and health planner. This is going to improve
the quality of health analysis in the state. All this requires is a simple computer connected to
the internet.  Thus this has a great social responsibility. This is in
turn justifies why this project has a great social sustainability.

\subsection{Environmental Sustainability}
From the sections of temporal planning and the budget planning we understand that we have a
computer running throughout the project. If we make an assumption that the amount of
energy used by a single computer comes to around 250 watts. And given that we spend 500
hours on the project then the energy expended is 125KW.  On average, electricity sources emit 1.222lbs CO2 per kWh (0.0005925 metric tons = 0.53750695845 Kg of CO2 per kWh). (Source: EPA eGRID Summary Tables and Data Files). This amounts to a upper bound of 67.1884 kg of
$CO_2$ considering that the energy was produced by using coal lignite. This can be reduced
by reducing the code size which is possible by reusing the already existing code. But the project
is actually environmentally sustainable.

\chapter{Background Knowledge}
In this section we present the background knowledge required to understand and solve the problem
\section{Graph Notation}

\subsubsection{Graph Definition}
Graph ,\textit{G} , is construct consisting of two finite sets, the set \textit{V} = \{ $v_1,v_2, \ldots ,v_n$ \} of vertices and the set \textit{E} = \{ $e_1,e_2, \ldots,e_n$  \} of edges where each edge is a pair of vertices from \textit{V}, for instance,
\begin{center}
$e_i = (v_j,v_k)$
\end{center}
is an edge from $v_j$ to $v_k$ represented as \textit{G}=(\textit{V},\textit{E}). In other words  E $\subset V^2$, which is the set of all unordered. The vertices ($v_j$ and $v_k$) that represent an edge are called \textit{endpoints} and the edge is said to be adjacent to each of its end points.  The definition has been framed accordint to the book on An "Introduction to Formal Languages and Automata" \cite{PeterLinz}

\begin{figure}[h]
\caption{Example Graph G=(V,E) Bull Graph}
\includegraphics[scale=0.5]{bull.png}
\centering
\end{figure}

The neighbourhood of a node $v_i$ is the set of
nodes $v_i$ is connected to, N($v_i$) = $\{v_j | (v_i, v_j) \in E, v_i \neq
v_j, 1 \leq j \leq n\}$. The degree of a node $v_i$, or the size of
the neighbourhood connected to $v_i$, is denoted as d($v_i$) =
$|N(v_i)|$. 
\par A degree sequence, \textit{D}, specifies the set of all node
degrees as tuples, such that D = {($v_i, d(v_i)$} and follows a
probability distribution called the \textit{degree distribution} with
mean $d_m$. \cite{githubtest1}

\subsection{Graph Matrix Notation}
Graphs can be appropriately represented in the form of matrices for instance, adjacency matrix, admittance matrix etc.,
\\
Let \textit{G}=(\textit{V},\textit{E}) be a simple graph with vertex set \textbf{V} and edge set \textbf{E}, then the adjacency matrix is square $|V|^2$ matrix \textbf{M} such that its element $M_{i,j}$ is \textit{1} when there is an edge from $v_i$ to $v_j$,where $v_i \in \textbf{V}$ , $ v_j \in \textbf{V}$ and \textit{0} when there is no edge.
The adjacency matrix of a graph of order \textit{n} entitles the entire the topology of a graph.  The diagonal elements of the adjacency matrix are all \textit{0} for undirected graphs \textbf{M}.
\par The sum of the elements of \textit{i}-th row or column yields the degree of node \textit{i}. If the edges are weighted, one defines the weight matrix \textbf{W}, whose element \textit{W}$_{ij}$ expresses the weight of the edges between vertices \textit{i} and \textit{j}.

\par The \textit{spectrum} of a graph \textbf{G} is the set of eigenvalues of it's adjacency matrix \textbf{M}. If \textbf{D}  is the diagonal matrix whose element \textit{D} $_{i,i}$ equals the degree of vertex i ($v_i \in$ \textit{V}).

\subsection{Approaches}
In this section we discuss the various approaches that are involved in dealing with the input to the project for community identification, for clustering and for visualization purposes. 
\subsubsection{For Community Identification}
Virtually in every scientific field dealing with empirical data, primary approach to get a first impression on the data is by trying to identify groups having "similar" behaviour in data. There are numerous methods to achieve this objective of which 

\begin{itemize}
\item Community Detection
\item Clustering
\end{itemize}

\paragraph{Community Detection}
\paragraph{Definition of a Community}
Communities are a part of the graph that has fewer ties with the rest of the system. Community detection traditionally focuses on the graph structugmres while clustering algorithms focuses on node attributes. 

Several tyoes of community detection algorithms can be distinguished
\paragraph{Divisive algorithms}
Divisive algorithms detect inter-community links and remove them from the network

\paragraph{Agglomerative algorithms}
Agglomerative algorithm merges similar nodes or communities in a recursive manner.

\paragraph{Optimization Methods}
Optimization methods are mainly based on maximization of an objective function.




\subsubsection{Clustering}
According to the paper "Community detection in graph" \cite{communitypaper} there are 4 major traditional clustering methods namely : 
\begin{itemize}

\item Graph Partitioning 

\item Hierarchical Clustering

\item Partitional Clustering

\item Spectral Clustering

\end{itemize}

\paragraph{Graph Partitioning}
This problem deals with dividing graph into groups of predefined size such that the number of edges between the groups is minimized. The paper \cite{communitypaper}  also defines cut size as the number of edges lying between the clusters.  Figure ~\ref{grapar} shows a problem with 14 vetices and presents a solution for spliting into 2 groups. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{graphpart.png}
\caption{\label{grapar}Graph Paritioning \cite{communitypaper}}

\end{figure}
\par \textit{Minimum Bisection Problem}, is a special problem case that considers partitioning the network into 2 groups of equal size. This problem is an NP-Hard problem. Intutively, to obtain ful partitioning we need to iteratively find all the minimum partition. This is not of significant use in the current problem of finding communities. 
\paragraph{Hierarchical Clustering}
Hierarchical clustering aims to identify groups of vertices with high similarities. It can be calssifies into two categories:
 \begin{enumerate}
\item \textit{Agglomerative algorithm} : in one in which Agglomerative algorithms, in which clusters are iteratively merged if their similarity is sufficiently
high
\item \textit{Divisive algorithms}, in which clusters are iteratively
split by removing edges connecting vertices with
low similarity.
The figure ~\ref{herar} demonstrates the hierarchcal clustering in a diagramatic manner.
\end{enumerate} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{hirac.png}
\caption{\label{herar}From a thickly knit graph to a dendogram}
\end{figure}
\paragraph{Partitional Clustering}

\paragraph{Spectral Clustering}





\subsubsection{For  Visualization}
Graph visualization is a important task in various scientific application. Visualization of data as graphs Visualizing these data
as graphs provides the non-experts with an intuitive means
to explore the content of the data, identify interesting patterns,
etc. Such operations require interactive visualizations
(as opposed to a static image) in which graph elements are
rendered as distinct visual objects; e.g., DOM objects in a
web browser. This way, the user can manipulate the graph
directly from the UI, e.g., click on a node or an edge to
get additional information (metadata), highlight parts of the
graph, etc. Given that graphs in many real-world scenarios
are huge, the aforementioned visualizations pose significant
technical challenges from a data management perspective

\paragraph{Web UI and Time per Query}
\begin{enumerate}
\item \textit{Program execution and Computation} : 
\item \textit{Building the JSON Object}
\item \textit{Communication Time}
\item \textit{Rendering }
\end{enumerate}
The total time is the sum of all the above times. 
\subsubsection{Computational Complexity}
 The estimate of the amount of resources required for by the algorithm to perform a task is defined as computational complexity. The humongous amount of data on the real graphs or real networks that are available in the current scenario causes the efficiency of the clustering algorithm to be crucial.
\par In a brief, Algorithms that have polynomial complexity describe the Class \textbf{P}. Problems whose solutions can be verified in a polynomial time span the class \textbf{NP} of \textit{non--deterministic polynomial time} problems, which includes \textbf{P}. problem is \textbf{NP}-hard if a solution for it can be
translated into a solution for any \textbf{NP}-problem. However,
a \textbf{NP}-hard problem needs not be in the class \textbf{NP}. If it
does belong to \textbf{NP} it is called \textbf{NP}-complete. The class
of \textbf{NP}-complete problems has drawn a special attention
in computer science, as it includes many famous problems like the Travelling Salesman, Boolean Satisfiability
(\textbf{SAT}), Integer Programming, etc.
The fact that \textbf{NP} problems have a solution which is verifiable in polynomial
time does not mean that NP problems have polynomial
complexity, i. e., that they are in \textbf{P}. In fact, the question of whether \textbf{NP}=\textbf{P} is the most important open problem in theoretical computer science. \textbf{NP}-hard problems
need not be in \textbf{NP} (in which case they would be \textbf{NP}-complete), but they are at least as hard as \textbf{NP}-complete
problems, so they are unlikely to have polynomial complexity, although a proof of that is still missing.
\par Many clustering Algorithms or problems related to clustering are \textbf{NP}-hard. This makes it irrelavant to use the exact algorithm, in which case we use an approximation algorithm. Approximation algorithm are methods that do not deliver the exact solution but an approximate solution but with an advantage of lower complexity. \cite{communitypaper}

\subsection{State-of-the-art in Community Detection}
Modularity is the objective function that is widely used both as a measure and as a optimzing method for partitioning community.
As said before there are various algorithms that can be used for community detection . In reference to the paper \cite{generalcommunity} which discusses 6 different community detection algorithms namely: 
\begin{itemize}
\item Louvain Method
\item Le Martelot
\item Newman’s greedy algorithm (NGA)
\item Newman’s spectral algorithm with refinement
\item simulated annealing
\item extremal optimization
\end{itemize}
The following figure ~\ref{Fig1} the average normalized performance rank of each algorithm in terms of partitioning quality and speed. Taken from the paper that proposed Combo algorithm \cite{generalcommunity}. 
\begin{figure}[h]
\includegraphics[scale=0.5]{lou.png}
\centering
\caption{\label{Fig1}Average normalized performance rank of each algorithm in terms of partitioning quality and speed}

\end{figure}
The main Objective of the project is to visuvalize the data on screen thus needs an algorithm that is fast and should be effective.  Hence louvain algorithm was choosen for the implementation. The implementation can be found in the later scection of the project.


Louvain algorithm algorithm is considered as state-of-the art algorithm for community detection\cite{louvain}. The algorithm is fast,recursive and is more effective on real world graphs. Oweing to our object of projecting medical domain we require an algorithm that gives a better trade off between being effective and being fast. Hence Louvain algoithm was choosen.


\subsection{State-of-the-art in Graph Visualization}

\subsection{Degree Distribution}
Degree of a node($v_i$) in the graph G = (V,E) where V is the set of vertices and E is the set of edges is the number of edges that a  node has to other nodes. Usually denoted as deg($v_i$). \textit{Degree distribution} can be thus described as the probability distribution of there degrees over the entire graph.  Degree distribution is significant in the study of community networks and hence bringing it into consideration.  It is usually denoted as \textit{P(k)} of a graph which is the fraction of nodes in the network with degree \textit{k}.
\begin{equation}
\begin{split}
P(k) &= \frac{N_k}{N}
\end{split}
\end{equation}
where $N_k$ is the number of  nodes with degree \textit{k} and \textit{N} is the total number of nodes in the graph.
\subsubsection{Scale-Free Graph}
The graphs whose degree distribution follows power law are called as Scale-Free graphs or scale free networks. 
\\
Examples of Scale-Free graphs include Social network graph,protein-protien interation network etc.

\chapter{Community Detection Algorithm}
\section{Introduction}
In this section we would describe the community detection algorithms such as louvain and various tests that were performed to choose the algorithm. 
\section{Louvain Algorithm}
\subsection{Introduction}

The problem of community detection requires the graph to be split into communities of tightly packed or in other words densely connected nodes with nodes of different community being sparely connected. 

Several algorithms have been proposed for perfoming good partition in a reasonably good speed.
Distinguishably there are several types of community detection algorithms, namely: divisive algorithms, which aims in removal of the inter-community links, agglomerative algorithms, which aim in merging similar nodes and optimization methods which aim in maximzing the objective function. 
\subsection{Modularity}
The quality of partitioning that results from application of method is often measured using modularity. The \textit{modularity} of a partition is hence a scalar value between -1 and 1 that is used to measure the density of the links inside the communities as compared to the density of the links between the communities.
\par
Modularity not only serves as a quality measure for detecting the quality of split or partition, but also acts as an objective function to optimize.  Exact modularity optimization is \textbf{NP-Complete} in the strong sense \cite{modularityNP}.
\subsubsection{Definition}
Let G=(V,E) be a simple graph,where \textit{V} is the set of vertices and \textit{E} is the set of undirected edges. Let $n=|V|$ and $m=|E|$. Let degree of a vertex \textit{v} be, deg(\textit{v}) where \textit{v} $\in$ V. Let C be the community, $C \subseteq V$, be the subset of vertices. A \textit{clustering} $C_s =\{C_1,C_2, \ldots, C_k\}$ of G is a partition of V such each vertex is present exactly in one cluster.  We thus define \textit{modularity } as follows: \cite{modularityNP}

\begin{equation} \label{eq1}
\begin{split}
Q(C_s) &= \sum_{C \in C_s}\left[ \frac{|E(C)|}{m} - \left( \frac{|E(C)+\sum_{k \in C_s}|E(C,k)|}{2m} \right)^2 \right]
\end{split}
\end{equation}
where E(I,J) is set of all edges between vertices in cluster I and J. E(C) = E(C,C).
The above equation can be continently rewritten as follows: 
\begin{equation} \label{eq1}
\begin{split}
Q(C_s) &= \sum_{C \in C_s}\left[ \frac{|E(C)|}{m} - \left( \frac{\sum_{v \in C}deg(v)}{2m} \right)^2 \right]
\end{split}
\end{equation}
In simpler terms the value is of Q can be expressed as 
\begin{equation} \label{eq1}
\begin{split}
Q &= \left( \text{Number of Intra-Cluster Communities} \right) - (\text{Expected number of Edges})
\end{split}
\end{equation}

As given in \cite{louvain}

\begin{equation}
    \begin{split}
Q &= \frac{1}{2m} \sum_{ij} \left(A_{ij} - P_{ij}\right) \delta(C_i,C_j)
\end{split}
    \end{equation}

\begin{equation}
\begin{split}
\delta(C_i,C_j) &= \begin{cases} 1 ,& if C_i = C_j \\
 0,& otherwise
 \end{cases}
\end{split}
\end{equation}

where, $P_{ij}$ is the expected number of edges between nodes $v_i$ and $v_j$. $P_ij$ is $\frac{k_ik_j}{2m}$ where $k_x$ is sum of the weights of the edges attached to the vertex $v_x$ for a given random graph G (This is otherwise called as a null model).
    
\subsubsection{Properties of Modularity}
\begin{enumerate}
 \item Q depends on nodes in the same clusters only. 
  \item Larger modularity implies better Communities.
    \item 
    \begin{equation}
    \begin{split}
Q(C_s) \leq \frac{1}{2m} \sum_{ij} A_{ij} \delta(C_i,C_j) \leq \frac{1}{2m} \sum_{ij} A_{ij} \leq 1
\end{split}
    \end{equation}
 \item Value taken by Q can be negative
 \end{enumerate}
 
 \subsection{Louvain}
 Louvian algorithm is considered as the state-of-the art algorithm for community detection for identifying community structures \cite{louvain}. Louvain algorithm consists of two phases:


\subsection{Implementation}

The implementation  of the algorihtm is based on the paper "Fast unfolding of communities in large networks" \cite{louvain}.  The implementation is done using basic python packages. 
 The Algorithm has two phases that are repeated iteratively to bring the final solution to the problem. The following figure ~\ref{loupic} demonstrates the algorithm in the form of a flow diagram,
 
\begin{algorithm}[H]
\caption{Louvain Algorithm Pseudocode}
\begin{algorithmic} 
\REQUIRE A graph G = (V,E)
\ENSURE Local optimum community split has happened

\WHILE{$Local Optimum Reached$}
\STATE Phase1 : Split or partition the graph by optimizing modularity greedily
\STATE Phase2 : Agglomerate the found clusters into new nodes
\ENDWHILE
\end{algorithmic}
\end{algorithm}


 \begin{figure}[h]

\includegraphics[scale=0.5]{loustep.png}
\caption{\label{loupic} Visualization of the steps of our algorithm. Each pass is made of two phases:
one where modularity is optimized by allowing only local changes of communities;
one where the found communities are aggregated in order to build a new network of
communities. The passes are repeated iteratively until no increase of modularity is
possible. This was taken from the paper "Fast unfolding of communities in large networks" \cite{louvain}}
\centering
\end{figure}

\subsubsection{First Phase : Optimizing Modularity}
The first phase of louvain algorithm 
Let G be a graph with N nodes in the network. The algorithm assigns a different community to each node in the network.  The number of nodes is equal to the number of communities in the graph. The report uses the terms node and vertices interchangeably.  Let $v_i$ be be a node such that $v_j$ $\in$ N($v_i$). The gain of modularity is then calculated by removing $v_i$ and placing it in community of $v_j$. If the gain is positive the $v_i$ is moved to the community of $v_j$ else $v_i$ stays in it's original community. This procedure is iterated and the phase one stops when a local maxima of the modularity is achieved, that is when no more move of nodes from one community to another is possible. The ordering of the nodes can affect or effect the computation time which can be a part of future works. 

\begin{algorithm}[H]
\caption{Phase 1 in Louvain Algorithm Pseudocode}
\begin{algorithmic} 
\REQUIRE A graph G = (V,E)
\ENSURE Partition network greedily using modularity
\STATE Assign a different community to each node
\WHILE{$Local Optimum Reached$}
\FORALL{Each node $v_i$}
\STATE{ For each node $v_j$ $\in$ N($v_i$), consider removing $v_i$ from community of $v_i$ and place it in the community of $v_j$}
\STATE Calculate the modularity gain
\IF {\textit{ModularityGain} is Positive}
\STATE remove $v_i$ from community of $v_i$ and place it in the community of $v_j$
\ELSE 
\STATE No Change
\ENDIF
\ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\par The main algorithm relies on the calculation of modularity. Listing 3.1 demonstrated the calculation using a python snippet. The first phase of the algorithm has been written into a python code snippet and is presented in the Listing 3.2. 
In the paper it is stated that the gain in modularity as $\bigtriangleup$Q

\begin{equation}
\begin{split}
\bigtriangleup Q &= \left[ \frac{\sum_{in} + k_{i,in}}{2m} - \left( \frac{\sum_{tot} + k_{i}}{2m} \right)^2 \right] - \left[ \frac{\sum_{in}}{2m} - \left( \frac{\sum_{tot}}{2m} \right)^2 - \left( \frac{k_i}{2m} \right)^2 \right]
\end{split}
\end{equation}
where $\sum_{in}$ is the sum of the weights of the links inside C and $\sum_{tot}$ is the sum of the weights of the links incident to nodes in C, $k_i$ is the sum of weights of the links incident to node i, $k_{i,in}$ is the sum of the weights  of all the links in the network.

\paragraph{Second Phase : Agglomerating the communities found in first phase into new nodes}
In the second phase the algorithm builds the new network. The communities that are found during the first phase are now the nodes here. According to the paper \cite{louvain}, the weights of the links between the new nodes are given by the sum of the weight of the links between nodes in the corresponding two communities. The edges between nodes of the same community lead to seld-loops for this community in the new network. The resulting new weighted network is then subjected to first phase  and this process is iteratively done. 
\begin{algorithm}[H]

\caption{Phase 2 in Louvain Algorithm Pseudocode}
\begin{algorithmic} 
\REQUIRE A graph G = (V,E)
\ENSURE Agglomeration of nodes
\STATE Every community $C_i$ forms a new node $v_i$
\STATE $W_{ij}$ = $\sum$ \{All edges between $C_i$ and $C_j$\} where $W_{ij}$ is the edge between newly formed nodes $v_i$ and $v_j$
\end{algorithmic}
\end{algorithm}
\subsubsection{Observations of Louvain}
\begin{enumerate}
\item The final output of the louvain algorithm forms a complete hierachical structure.
\item Resolution limit problem \cite{ResolLimit} has been resolved in the algorithm stated in the current paper under discussion \cite{louvain} due to the multi-level nature of louvain algorithm.
\item Modularity can be redefined for weighted graphs and Louvain works well with weighted graphs.
\end{enumerate}

\subsubsection{Usage of Louvain in the project}
In the project the above algorithm has been implemented in python taking inspiration and reusing some part of the pylouvain program API for implementation \cite{pylouvain}. Python was chosen as the programming language as recommended by the project Director. The project work is said to form a part of a major project work on medical domain in LARCA. The director of the project informed that the main project is written in javascipt and python hence, python was choosen as the programming language for the project.

\subsection{Experiments}
\cite{githubtest1}
\\
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{e1000.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{e1000b.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{e2000.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{e2000b.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}


\subsubsection{Result}

\section{Matrix Based Algorithm}
\subsection{Matrix Algorithm}
\subsubsection{Introduction}
\subsubsection{Reasoning}
\subsubsection{Description}
\subsubsection{Implementation}
\subsubsection{Experiments}
\subsubsection{Result}

\chapter{Visualization Techniques}
In the field of \textit{Mathematics} and \textit{Computer Science} graph drawing is an area that uses both graph theory and infomation visualization to generate a 2-D depection of graphs. 


\section{Alchemy.js}
According to the creator of the Alchemy.js Software \cite{Alchemy}
Alchemy.js is a graph drawing software built on \textit{d3}. Alchemy.js is an application which provides graph visualization with very little over head.  
\subsection{Dependencies}
Alchemy needs 3 main units to  form as an application namely:  \textit{alchemy.css}, \textit{alchemy.js} and \textit{data}. CSS and JavaScript are major dependencies in Alchemy.js. Installation of \textit{jQuery} and \textit{d3} is also useful. \textit{alchemy.min.js}, \textit{alchemy.css} and \textit{alchemy.min.css } will be updated in the CDN (Content Delivery netwrok)
\subsection{Steps to use the Alchemy.js}
 In this project we have uploaded the \textit{alchemy.min.js}, \textit{alchemy.css} and \textit{alchemy.min.css } into the project's file repository http://abhinavsv3.github.io/javascriptsal and hence will be using the link in the following explanation. 
The following describes the steps that are followed in use of Alchemy.js :
\begin{enumerate}

\item \textit{Include the files in this format}\\
$<$link rel="stylesheet" href="http://abhinavsv3.github.io/javascriptsal/alchemy.min.css"/$>$
\\
\ldots\\
$<$script src="http://abhinavsv3.github.io/javascriptsal/alchemy.min1.js"$></script>$

\item \textit{Include an element with "alchemy" ID as the id and class }\\
The alchemy class is used to apply styles while the alchemy id is used programatically. By default Alchemy.js looks for the alchemy div but this can be overridden.
$<$div class="alchemy" id="alchemy"$><$/div$>$

\item \textit{Provide Alchemy.js with a JSON dataSource}

\item \textit{Begin Alchemy.js}
$<$script$>$
  alchemy.begin$(\{"dataSource": someData\})$
$<$/script$>$



\end{enumerate}
\section{Getting the data from the Louvain Python code to Alchemy }
According to the authors of Alchemy.js. Alchemy.js takes a simple data format called GraphJSON. GraphJSON serves as a light weight and flexible representation of graph data, easily consued locally or over the web. 
\par 
GraphJSON is a JSON Object which has 2 object namely: \textit{nodes} and  \textit{edges} . These are induvidual arrays that represent the nodes and edges that will be represented in the graph visualization. 
\par \textbf{Nodes} : id key is the only unique value that should be present in the nodes
\par \textbf{Edges} : source and target key are the only unique value that should be present in the edges.

\subsubsection{Tests}
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t1.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t2.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t3.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t4.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t5.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t6.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{t7.png}
\caption{Example Graph G=(V,E) Bull Graph}
\end{figure}


\subsubsection{Result}

\chapter{Overall System Description}
The project uses two inistingusable technologies of a raw python code and a JavaScript program that can represent or draw the output of the python code on screen. This lead to a need to include a new element of a web framework that helps in combining both the tasks.  A web framework is one that aims to allevate the overhead associated with common activities performed in the web development. 
\section{Choice of Web.py}
An exploration one a few python web frameworks such as Django, Grok and web.py. A sample application in Django was built to see if Django suits the need of the project. Django was eliminated oweing to the fact that it was heavier for a simple task that we wish to perform in the project. Distinguishing the features of both Grok and web.py. Web.py was chosen as the web framework for the project as it allowed successful integration of the existing python code with the web framework. 
The auxiliary main project was more associated with building a web project based on Angular.js , which was also the suggestion of the project director. But web.py commanded the current need of the project that is to perform the integration task.  
\section{Front End frame work}
Bootstrap was an intuitive web front-end frame work that has been implemented in the project.  Bootstrap can be divided into various matrix cubes enabling us to place buttons to run the application.



\section{Using the application}
The web application thus developed is intutive enough such that even though it performs tasks of computation and integration into the web it looks simple. Minimal and most necessary elements have been implemented.
In the following we describe the entire working flow of the Web Application:
\begin{enumerate}
\item \textbf{"Login Page"} In this part a simple login password is described.  
\item \textbf{File Loader Page} After logining the user lands in a file loader page where provision has been provided to upload a text file containing the adjacency matrix of the graphs are present in the "start" "Destination" or "Start" "Weight" "Destination" format. In this space only numerical values can be accepted for supporting the simplity of the background process. Another space to provide a key that maps the names of the node numbers. This helps to present another dimention of seeing the data in the form of names.  Viewing the names of the node over nodes helps the user to deepen his vision of analysis. 
\item \textbf{Dash Board} In this part of the web application the user lands in a page that presents to the user 4 different variety of visual representation of the input data :  Community graphs (Diplaying the links between various communities), Full graph split into communities represented with different colors, Original graph input(Helps to see if the input was proper) and a Matrix view of the graph
\item \textbf{On Click Viz.} On clicking on the visualization the user wants to take the web aplication switches to the graph that the user has clicked.
\item \textbf{Error Page} In case of mismatch in the format or the password was wrong or the visualization is not possible due the screen size or the browser is not able to handle the large JSON object Errors pages have been genrated to couter act on exceptions. 
\end{enumerate}
\begin{tabular}{|c|c|}
      \hline
      \addheight{\includegraphics[width=60mm]{s1.png}} &
      \addheight{\includegraphics[width=60mm]{s3.png}} \\
      \small ``row 2, column 1'' &  ``row 2, column 2'' \\
      \hline
\end{tabular}
\\

\begin{tabular}{|c|c|}
      \hline
      \addheight{\includegraphics[width=60mm]{s4.png}} &
      \addheight{\includegraphics[width=60mm]{serr.png}} \\
      \small ``row 2, column 1'' &  ``row 2, column 2'' \\
      \hline
\end{tabular}


\subsubsection{Implementation Benefits}
sdgbf akjsfkldsnfksndafnadfa
fasfadf
adfasfasfadf
asfasfadf
afadfadf
\subsubsection{Description}
git hub repository  : \url{https://github.com/abhinavsv3/webproject}
\subsubsection{Result}
\subsection{Benefits to the community}
This can be used in places where there is difficulty in visualization of a very complex landscape
of data such as medical domain. In Medical domain a patient can be a vector of diseases and
visualization of such patients (patients graph–which shows relations of how two patients are
similar, a graph in which patient-patient edge weight is the similarity value ) would be useful for
analyzing and predicting the disease landscape of a region and in turn multiple regions.

\chapter{Conclusion and Future Works}
\section{Goals Achieved}
\section{Revision of Planning and Budget}
\section{Future Works}
Order of inputs can influence the computation time. The problem of finding specific heuristics to solve this ordering problem can improve the louvain algorithms computation time.
Zooming effect
merging the dashboard to the graph board.
The project relies fully on louvain. One can speculate on whether modularity is the only measure that exists. THinking about a completely new measure would be interesting. 
Database for passwords. 
\section{Availability and requirements}
\begin{enumerate}
\item \textbf{Project Name}: Graph and matrix algorithms for visualizing high dimensional
\item \textbf{Project Homepage}: \url{https://github.com/abhinavsv3/webproject dimensional}
\item \textbf{Operating System}: Platform Independent. Preferably Unix--like operating system
\item \textbf{Programming Language}: Python 2.7
\item \textbf{Other Requirements} : Alchemy.js, Python Packages, Web.py
\end{enumerate}

\subsection{Conclusion}
This is one of the greatest project experience.	
\subsection{Personal Conclusion}
This is one of the greatest project experience.	



\lstlistoflistings


\bibliographystyle{plain}
\bibliography{mybib}{}
\end{document}



